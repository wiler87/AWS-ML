{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<center><img src=\"images/logo.png\" alt=\"AWS Logo\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
        "\n",
        "# Application of Deep Learning to Text and Image Data\n",
        "## Module 1, Lab 2: Creating a Multilayer Perceptron and Dropout Layers\n",
        "\n",
        "In this notebook, you will implement a simple neural network with multiple layers and analyze the training process. You will then implement dropout layers to prevent overfitting of the neural network.\n",
        "\n",
        "**Multilayer perceptron**\n",
        "\n",
        "The simplest feed-forward neural network architecture is a multilayer perceptron (MLP). An MLP is characterized by several layers of input neurons that are fully connected. Forming an MLP requires at least three layers: input layer, hidden layer, and output layer. An MLP uses backpropagation to train the network.\n",
        "\n",
        "**Dropout layers**\n",
        "\n",
        "To prevent overfitting of neural networks, it's possible to randomly drop a certain percentage of the neurons (or nodes) in the input and hidden layers. This has proven to be an effective technique for regularization and preventing the coadaptation of neurons (for neurons that show correlated behavior). The dropout layer only applies during training of the neural network. Neurons aren't dropped when making predictions (inference).\n",
        "\n",
        "You will learn the following:\n",
        "\n",
        "- How to define a single dense-layer neural network model\n",
        "- How to train the neural network\n",
        "- Why dropout layers are important\n",
        "- How to add a dropout layer\n",
        "\n",
        "---\n",
        "\n",
        "You will be presented with activities throughout the notebook: <br/>\n",
        "\n",
        "|<img style=\"float: center;\" src=\"./images/activity.png\" alt=\"Activity\" width=\"125\"/>| \n",
        "| --- | \n",
        "|<p style=\"text-align:center;\"> No coding is needed for an activity. You try to understand a concept, <br/>answer questions, or run a code cell.</p>|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Index\n",
        "\n",
        "* [Dataset](#Dataset)\n",
        "* [Define the model](#Define-the-model)\n",
        "* [Train the neural network](#Train-the-neural-network)\n",
        "* [Add a dropout layer](#Add-a-dropout-layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9UvVPODTpwj",
        "origin_pos": 4
      },
      "source": [
        "---\n",
        "## Dataset\n",
        "\n",
        "The [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset consists of 28x28 (=784) pixel grayscale images from 10 categories. The dataset has 6,000 images in each category for the training dataset and 1,000 in each category for the test dataset. \n",
        "\n",
        "Your task is to build a classifier that maps the images to their categories. You will use PyTorch predefined layers and the default trainers for a swift and efficient implementation of an MLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install libraries\n",
        "!pip install -U -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Import system library and append path\n",
        "import sys\n",
        "sys.path.insert(1, \"..\")\n",
        "\n",
        "# Import utility functions that provide answers to challenges\n",
        "from MLUDTI_EN_M1_Lab2_quiz_questions import *\n",
        "\n",
        "# Import PyTorch library and plotting library\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from torch.utils import data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the image dataset with the torch helper functions\n",
        "\n",
        "mnist_train = torchvision.datasets.FashionMNIST(\n",
        "    root=\"data\", train=True, transform=transforms.ToTensor(), download=True\n",
        ")  # ToTensor converts the image data from PIL type to 32-bit floating point tensors.\n",
        "\n",
        "mnist_val = torchvision.datasets.FashionMNIST(\n",
        "    root=\"data\", train=False, transform=transforms.ToTensor(), download=True\n",
        ")  # ToTensor converts the image data from PIL type to 32-bit floating point tensors.\n",
        "\n",
        "# Pass batches of images to the neural network\n",
        "batch_size = 256\n",
        "\n",
        "# To load images in batches, you need the DataLoader helper function\n",
        "training_loader = data.DataLoader(mnist_train, batch_size, shuffle=True)\n",
        "validation_loader = data.DataLoader(mnist_val, batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look at some of the images to see what is in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# To display the images, you need a function that plots them\n",
        "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
        "    \"\"\"Plot a list of images.\"\"\"\n",
        "    figsize = (num_cols * scale, num_rows * scale)\n",
        "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "    axes = axes.flatten()\n",
        "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
        "        ax.imshow(img.permute(1, 2, 0).numpy(), cmap=\"gray\")\n",
        "        ax.axes.get_xaxis().set_visible(False)\n",
        "        ax.axes.get_yaxis().set_visible(False)\n",
        "        if titles:\n",
        "            ax.set_title(titles[i])\n",
        "    return axes\n",
        "\n",
        "# You can update the num_rows and num_cols variables to change the number of images that are displayed\n",
        "for data, label in training_loader:\n",
        "    show_images(data, 4, 4)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Define the model\n",
        " \n",
        "Now that you have imported and reviewed the data, you need to build a linear model with a single dense layer that takes in a vector of length 784 (the number of input features) and returns another vector of length 10 (the number of output classes). Remember that you need to initialize weights and biases to get a first prediction and evaluation of the output that the MLP produces. A good starting point is to use a normal distribution for weights and zeros for biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Specify how many classes to predict (this needs to match the labels)\n",
        "in_features = 784\n",
        "out_classes = 10\n",
        "\n",
        "# Single-layer network; flatten is required because you are working with images, and each row should represent one data point.\n",
        "mlp = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(\n",
        "        in_features, out_classes\n",
        "    ),  # Use CrossEntropyLoss later with SoftMax built in, so no need to add here\n",
        ")\n",
        "\n",
        "# Initialize the network\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.normal_(m.weight, std=0.01)\n",
        "        nn.init.zeros_(m.bias)\n",
        "\n",
        "mlp.apply(init_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
        "    <h3><i>Try it yourself!</i></h3>\n",
        "    <br>\n",
        "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
        "    <p style=\" text-align: center; margin: auto;\">To test your understanding of neural net architectures, run the following cell.</p>\n",
        "    <br>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Run this cell to display the question and check your answer\n",
        "question_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Display the initial values of the w and b\n",
        "weight, bias = list(mlp.parameters())\n",
        "\n",
        "# Print weight and bias tensors\n",
        "print(\"Weights:\")\n",
        "print(weight)\n",
        "print(\"\\nBiases:\")\n",
        "print(bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "Now that everything is set up, test how well the untrained network performs when making predictions on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Look at 10 predictions in the first batch of data in the training loader\n",
        "for i, (data, label) in enumerate(training_loader):\n",
        "    pred = mlp(data)\n",
        "    print(\"Predictions:\")\n",
        "    print(torch.softmax(pred, dim=1).argmax(axis=1)[:10])\n",
        "    print(\"\\nTrue labels:\")\n",
        "    print(label[:10])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the model appears to be randomly guessing. Think about why the predictions are random.\n",
        "\n",
        "You might recall that you generated a normal distribution for weights and set the biases to zero. Those values have not been updated because you have not performed any training yet. While the code cell above doesn't create good predictions, you can use it to verify that the general architecture of the model works.\n",
        "\n",
        "Now you are ready to train the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MgNio3CZTpw6",
        "origin_pos": 25
      },
      "source": [
        "---\n",
        "## Train the neural network\n",
        "\n",
        "The training loop is similar to what you built in the previous lab. The main difference is that you will use `torch.optim` to complete the optimization algorithm. You will learn about different optimizers later in the course. For now, use the well-known stochastic gradient descent (SGD)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Determine if a GPU resource is available; otherwise, use CPU.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# This is a multiclass classification, so you want to use nn.CrossEntropyLoss.\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, you need to write a function to train the neural network. When you imported the data, you broke it into batches, so you need to include a loop for the training batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Function to train the network\n",
        "\n",
        "def train_net(net, train_loader, val_loader, num_epochs=1, learning_rate=0.1):\n",
        "    # Define the optimizer, SGD with learning rate\n",
        "    optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Initialize loss and accuracy lists\n",
        "    train_losses, train_accs, val_accs = [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        net = net.to(device)\n",
        "\n",
        "        # Initialize loss and accuracy values\n",
        "        train_loss, val_loss, train_acc, val_acc = 0.0, 0.0, 0.0, 0.0\n",
        "\n",
        "        # Training loop: (with autograd and trainer steps)\n",
        "        # This loop trains the neural network (weights are updated)\n",
        "        for i, (data, label) in enumerate(train_loader):\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            output = net(data)\n",
        "            loss = criterion(output, label)  # Compute the total loss in the train batch\n",
        "            loss.backward()\n",
        "            train_acc += (output.argmax(axis=1) == label.float()).float().mean()\n",
        "            train_loss += loss\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation loop:\n",
        "        # This loop tests the trained network on the dation dataset. No weight updates here.\n",
        "        for i, (data, label) in enumerate(val_loader):\n",
        "            data = data.to(device)\n",
        "            label = label.to(device)\n",
        "            output = net(data)  # Compute the total loss in the validation batch\n",
        "            val_acc += (output.argmax(axis=1) == label.float()).float().mean()\n",
        "            val_loss += criterion(output, label)\n",
        "\n",
        "        # Take averages\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "        val_acc /= len(val_loader)\n",
        "\n",
        "        train_losses.append(train_loss.item())\n",
        "        train_accs.append(train_acc.item())\n",
        "        val_accs.append(val_acc.item())\n",
        "\n",
        "        print(\n",
        "            \"Epoch %d: train loss %.3f, train acc %.3f, val loss %.3f, val acc %.3f\"\n",
        "            % (\n",
        "                epoch + 1,\n",
        "                train_loss.detach().cpu().numpy(),\n",
        "                train_acc.detach().cpu().numpy(),\n",
        "                val_loss.detach().cpu().numpy(),\n",
        "                val_acc.detach().cpu().numpy(),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return train_losses, train_accs, val_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you have created a training function, use it to train the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Train the neural network\n",
        "train_losses, train_accs, val_accs = train_net(\n",
        "    mlp, training_loader, validation_loader, num_epochs=25, learning_rate=0.03\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After training finishes, you can create plots of the training loss, training accuracy, and validation accuracy. This will help you determine how well your model is performing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define a function to plot the training losses\n",
        "def plot_losses(train_losses, train_acc, val_acc):\n",
        "\n",
        "    plt.plot(train_losses, label=\"Training Loss\")\n",
        "    plt.title(\"Loss values\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(train_acc, \"g\", label=\"Train Accuracy\")\n",
        "    plt.plot(val_acc, \"red\", label=\"Validation Accuracy\")\n",
        "    plt.title(\"Accuracy values\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Plot the training loss function and accuracy\n",
        "plot_losses(train_losses, train_accs, val_accs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you look at the graphs, think about the following questions.\n",
        "1. What do you notice about the training loss? \n",
        "1. Was 25 epochs enough? \n",
        "1. Why is the validation accuracy lower than the training accuracy? \n",
        "1. Is the accuracy high enough to consider this a good model?\n",
        "\n",
        "What other questions do you have after reviewing the graphs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
        "    <h3><i>Try it yourself!</i></h3>\n",
        "    <br>\n",
        "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
        "    <p style=\" text-align: center; margin: auto;\">To test your understanding of epochs and learning rate, run the following cell.</p>\n",
        "    <br>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Run this cell to display the question and check your answer\n",
        "question_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Add a dropout layer\n",
        "\n",
        "In this final step, you will add a dropout layer to prevent overfitting. A dropout layer randomly drops a certain percentage or number of neurons in a given layer. You can specify how much to drop with `nn.Dropout`.\n",
        "\n",
        "Add another layer and a dropout layer after it to see how that affects the loss and accuracy values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Add a hidden layer and dropout layer in between\n",
        "mlp_dropout = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(784, 784),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(784, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(256, out_classes),\n",
        ")\n",
        "\n",
        "num_epochs = 25\n",
        "\n",
        "# Train the model by using the newly defined neural network\n",
        "train_losses, train_accs, val_accs = train_net(\n",
        "    mlp_dropout,\n",
        "    training_loader,\n",
        "    validation_loader,\n",
        "    num_epochs=num_epochs,\n",
        "    learning_rate=0.01,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Plot the loss function and accuracy graphs\n",
        "plot_losses(train_losses, train_accs, val_accs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you look at the graphs, think about the following questions.\n",
        "1. How do they compare to your original model without the dropout layer?\n",
        "1. Is the accuracy of the new model better?\n",
        "1. How does this impact the number of epochs that you need?\n",
        "1. Does changing any of the settings (such as the dropout, learning rate, or epochs) improve the accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
        "    <h3><i>Try it yourself!</i></h3>\n",
        "    <br>\n",
        "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
        "    <p style=\" text-align: center; margin: auto;\">To test your understanding of dropout layers, run the following cell.</p>\n",
        "    <br>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Run this cell to display the question and check your answer\n",
        "question_3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "----\n",
        "## Conclusion\n",
        "\n",
        "In this notebook, you learned how to build a more advanced neural network. Topics such as dense networks and dropout layers should start to make more sense as you build more understanding about building models.\n",
        "\n",
        "--- \n",
        "## Next lab\n",
        "In the next lab, you will learn how to build an end-to-end neural network."
      ]
    }
  ],
  "metadata": {
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "conda_pytorch_p310",
      "language": "python",
      "name": "conda_pytorch_p310"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

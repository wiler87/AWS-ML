{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/logo.png\" alt=\"drawing\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "# Application of Deep Learning to Text and Image Data\n",
    "## Module 1, Lab 3: Building an End-to-End Neural Network Solution\n",
    "\n",
    "In the previous lab, you used a neural network with image data to predict the category that an item belonged to. In this lab, you will process text data by building an end-to-end neural network solution. The solution will incorporate all the data processing techniques that you have learned so far. \n",
    "\n",
    "You will learn how to do the following:\n",
    "\n",
    "- Import and preprocess data.\n",
    "- Create a neural network with multiple layers.\n",
    "- Train text data with your neural network.\n",
    "- Validate your model as you train.\n",
    "- Change different parameters to improve your neural network.\n",
    "\n",
    "---\n",
    "\n",
    "__Austin Animal Center Dataset__\n",
    "\n",
    "In this lab, you will work with historical pet adoption data in the [Austin Animal Center Shelter Intakes and Outcomes dataset](https://www.kaggle.com/datasets/aaronschlegel/austin-animal-center-shelter-intakes-and-outcomes?resource=download). The target field of the dataset (**Outcome Type**) is the outcome of adoption: 1 for adopted and 0 for not adopted. Multiple features are used in the dataset.\n",
    "\n",
    "Dataset schema:\n",
    "- __Pet ID:__ Unique ID of the pet\n",
    "- __Outcome Type:__ State of pet at the time of recording the outcome (0 = not placed, 1 = placed). This is the field to predict.\n",
    "- __Sex upon Outcome:__ Sex of pet at outcome\n",
    "- __Name:__ Name of pet \n",
    "- __Found Location:__ Found location of pet before it entered the shelter\n",
    "- __Intake Type:__ Circumstances that brought the pet to the shelter\n",
    "- __Intake Condition:__ Health condition of the pet when it entered the shelter\n",
    "- __Pet Type:__ Type of pet\n",
    "- __Sex upon Intake:__ Sex of pet when it entered the shelter\n",
    "- __Breed:__ Breed of pet \n",
    "- __Color:__ Color of pet \n",
    "- __Age upon Intake Days:__ Age (days) of pet when it entered the shelter\n",
    "- __Age upon Outcome Days:__ Age (days) of pet at outcome\n",
    "\n",
    "---\n",
    "\n",
    "You will be presented with two kinds of exercises throughout the notebook: activities and challenges. <br/>\n",
    "\n",
    "| <img style=\"float: center;\" src=\"images/activity.png\" alt=\"Activity\" width=\"125\"/>| <img style=\"float: center;\" src=\"images/challenge.png\" alt=\"Challenge\" width=\"125\"/>|\n",
    "| --- | --- |\n",
    "|<p style=\"text-align:center;\">No coding is needed for an activity. You try to understand a concept, <br/>answer questions, or run a code cell.</p> |<p style=\"text-align:center;\">Challenges are where you can practice your coding skills.</p> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Index\n",
    "\n",
    "* [Data processing](#Data-processing)\n",
    "* [Training and validation of a neural network](#Training-and-validation-of-a-neural-network)\n",
    "* [Testing the neural network](#Testing-the-neural-network)\n",
    "* [Improvement ideas](#Improvement-ideas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Data processing\n",
    "\n",
    "The first step is to process the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\n",
      "ERROR: Could not find a version that satisfies the requirement os (from versions: none)\n",
      "ERROR: No matching distribution found for os\n"
     ]
    }
   ],
   "source": [
    "# Install libraries\n",
    "!pip install -U -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'boto3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import the dependencies\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m path\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'boto3'"
     ]
    }
   ],
   "source": [
    "# Import the dependencies\n",
    "import boto3\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from MLUDTI_M1_Lab3_neural_network import NeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, read the dataset into a DataFrame and look at it. The data might look familiar because it was used in the labs of the Machine Learning through Application course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/austin-animal-center-dataset.csv\")\n",
    "\n",
    "print(\"The shape of the dataset is:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the first five rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "Now, perform the basic steps of exploratory data analysis (EDA) and look for insights to inform later ML modeling choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the data types and nonnull values for each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the column names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create lists that identify the numerical, categorical, and text features, and the target/label\n",
    "numerical_features = [\"Age upon Intake Days\", \"Age upon Outcome Days\"]\n",
    "\n",
    "categorical_features = [\n",
    "    \"Sex upon Outcome\",\n",
    "    \"Intake Type\",\n",
    "    \"Intake Condition\",\n",
    "    \"Pet Type\",\n",
    "    \"Sex upon Intake\",\n",
    "]\n",
    "\n",
    "text_features = [\"Found Location\", \"Breed\", \"Color\"]\n",
    "\n",
    "model_features = numerical_features + categorical_features + text_features\n",
    "model_target = \"Outcome Type\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: The **Pet ID** and **Name** features were omitted because they are irrelevant to the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "\n",
    "#### Cleaning numerical features\n",
    "\n",
    "Take a moment to examine the numerical features. Remember that the `value_counts()` function can give a view of the numerical features by placing feature values in respective bins. The function can also be used for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in numerical_features:\n",
    "    print(c)\n",
    "    print(df[c].value_counts(bins=10, sort=False))\n",
    "    df[c].value_counts(bins=10, sort=False).plot(kind=\"bar\", alpha=0.75, rot=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any outliers are identified as likely wrong values, dropping them could improve the histograms for the numerical values and could later improve overall model performance.\n",
    "\n",
    "Remove any values in the upper 10 percent for the feature, and then plot the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in numerical_features:\n",
    "    # Drop values beyond 90% of max()\n",
    "    dropIndexes = df[df[c] > df[c].max() * 9 / 10].index\n",
    "    df.drop(dropIndexes, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in numerical_features:\n",
    "    print(c)\n",
    "    print(df[c].value_counts(bins=10, sort=False))\n",
    "    df[c].value_counts(bins=10, sort=False).plot(kind=\"bar\", alpha=0.75, rot=90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cleaning text features\n",
    "\n",
    "Take a moment to examine the text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare cleaning functions\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\", \"in\"]\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "def preProcessText(text):\n",
    "    # Lowercase text, and strip leading and trailing white space\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.compile(\"<.*?>\").sub(\"\", text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.compile(\"[%s]\" % re.escape(string.punctuation)).sub(\" \", text)\n",
    "\n",
    "    # Remove extra white space\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def lexiconProcess(text, stop_words, stemmer):\n",
    "    filtered_sentence = []\n",
    "    words = text.split(\" \")\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(stemmer.stem(w))\n",
    "    text = \" \".join(filtered_sentence)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def cleanSentence(text, stop_words, stemmer):\n",
    "    return lexiconProcess(preProcessText(text), stop_words, stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ The text cleaning process can take a while to complete, depending on the size of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean the text features\n",
    "for c in text_features:\n",
    "    print(\"Text cleaning: \", c)\n",
    "    df[c] = [cleanSentence(item, stop_words, stemmer) for item in df[c].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train, validation, and test datasets\n",
    "\n",
    "Now that the data has been cleaned, you need to split the full dataset into training and test subsets by using sklearn's [train_test_split()](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. With this function, you can specify the following:\n",
    "\n",
    "- The proportion of the dataset to include in the test split as a number between 0.0-1.0 with a default of 0.25.\n",
    "- An integer that controls the shuffling that is applied to the data before the split. Passing an integer allows for reproducible output across multiple function calls.\n",
    "\n",
    "To help reduce sampling bias, the original dataset is shuffled before the split. After the initial split, the training data is further split into training and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(\n",
    "    df, test_size=0.15, shuffle=True, random_state=23\n",
    ")\n",
    "\n",
    "train_data, val_data = train_test_split(\n",
    "    train_data, test_size=0.15, shuffle=True, random_state=23\n",
    ")\n",
    "\n",
    "# Print the shapes of the training, validation, and test datasets\n",
    "print(\n",
    "    \"Train - Validation - Test dataset shapes: \",\n",
    "    train_data.shape,\n",
    "    val_data.shape,\n",
    "    test_data.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing with a pipeline and ColumnTransformer\n",
    "\n",
    "In a typical ML workflow, you need to apply data transformations, such as imputation and scaling, at least twice: first on the training dataset by using `.fit()` and `.transform()` when preparing the data to train the model, and then by using `.transform()` on any new data that you want to predict on (validation or test). Sklearn's [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) is a tool that simplifies this process by enforcing the implementation and order of data processing steps.\n",
    "\n",
    "In this section, you will build separate pipelines to handle the numerical, categorical, and text features. Then, you will combine them into a composite pipeline along with an estimator. To do this, you will use a [LogisticRegression classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "You will need multiple pipelines to ensure that all the data is handled correctly:\n",
    "\n",
    "   * __Numerical features pipeline:__  Impute missing values with the mean by using sklearn's `SimpleImputer`, followed by `MinMaxScaler`. If different processing is desired for different numerical features, different pipelines should be built as described for the text features pipeline.\n",
    "\n",
    "   * __Categoricals pipeline:__ Impute with a placeholder value (this won't have an effect because you already encoded the `nan` values), and encode with sklearn's `OneHotEncoder`. If computing memory is an issue, it is a good idea to check the number of unique values for the categoricals to get an estimate of how many dummy features one-hot encoding will create. Note the `handle_unknown` parameter, which tells the encoder to ignore (rather than throw an error for) any unique value that might show in the validation or test set that was not present in the initial training set.\n",
    "\n",
    "   * __Text features pipeline:__ With memory usage in mind, build three more pipelines, one for each of the text features. The current sklearn implementation requires a separate transformer for each text feature (unlike the numericals and categoricals).\n",
    "\n",
    "\n",
    "Finally, the selective preparations of the dataset features are then put together into a collective ColumnTransformer, which is used in a pipeline along with an estimator. This ensures that the transforms are performed automatically in all situations. This includes on the raw data when fitting the model, when making predictions, when evaluating the model on a validation dataset through cross-validation, or when making predictions on a test dataset in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline(\n",
    "    [\n",
    "        (\"num_imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\n",
    "            \"num_scaler\",\n",
    "            MinMaxScaler(),\n",
    "        ),  \n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess the categorical features\n",
    "categorical_processor = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"cat_imputer\",\n",
    "            SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "        ),  # Shown in case it is needed. No effect here because you already imputed with 'nan' strings.\n",
    "        (\n",
    "            \"cat_encoder\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "        ),  # handle_unknown tells it to ignore (rather than throw an error for) any value that was not present in the initial training set.\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocess first text feature\n",
    "text_processor_0 = Pipeline(\n",
    "    [(\"text_vectorizer_0\", CountVectorizer(binary=True, max_features=50))]\n",
    ")\n",
    "\n",
    "# Preprocess second text feature\n",
    "text_processor_1 = Pipeline(\n",
    "    [(\"text_vectorizer_1\", CountVectorizer(binary=True, max_features=50))]\n",
    ")\n",
    "\n",
    "# Preprocess third text feature\n",
    "text_processor_2 = Pipeline(\n",
    "    [(\"text_vectorizer_2\", CountVectorizer(binary=True, max_features=50))]\n",
    ")\n",
    "\n",
    "# Combine all data preprocessors (add more if you choose to define more)\n",
    "# For each processor/step, specify: a name, the actual process, and the features to be processed.\n",
    "data_processor = ColumnTransformer(\n",
    "    [\n",
    "        (\"numerical_processing\", numerical_processor, numerical_features),\n",
    "        (\"categorical_processing\", categorical_processor, categorical_features),\n",
    "        (\"text_processing_0\", text_processor_0, text_features[0]),\n",
    "        (\"text_processing_1\", text_processor_1, text_features[1]),\n",
    "        (\"text_processing_2\", text_processor_2, text_features[2]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Visualize the data processing pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "data_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "X_train = train_data[model_features]\n",
    "y_train = train_data[model_target].values\n",
    "\n",
    "# Get validation data to validate the network\n",
    "X_val = val_data[model_features]\n",
    "y_val = val_data[model_target].values\n",
    "\n",
    "# Get test data to test the network for submission to the leaderboard\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target].values\n",
    "\n",
    "print(\"Dataset shapes before processing: \", X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "X_train = data_processor.fit_transform(X_train).toarray()\n",
    "X_val = data_processor.transform(X_val).toarray()\n",
    "X_test = data_processor.transform(X_test).toarray()\n",
    "\n",
    "print(\"Dataset shapes after processing: \", X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training and validation of a neural network\n",
    "\n",
    "Now, run the following code cell to interact with the neural network to gain insight into how neural networks train.\n",
    "\n",
    "Architect the neural network by updating the number of layers (maximum of 4) and the number of neurons per layer (maximum of 3) to solve the classification problem that displays when you run the following cell. The background colors show the neural network's predicted classification regions for the true data (circles).\n",
    "\n",
    "Note that upon retraining the network, the weights are randomly initialized, and the gradients are reset to 0. In the visual representation, each green circle corresponds to an epoch. Each red circle corresponds to that layer's weight update gradient (from backpropagation).\n",
    "\n",
    "To develop a better understanding, train the model for different architectures. Note that the model gets stuck sometimesâ€”initialization is important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to build a PyTorch neural network and use it to fit to the training data. As part of the training, you need to use the validation data to check performance at the end of each training iteration.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\"text-align: center; margin: auto;\">To define the hyperparameters of the algorithm, run the following cell. Note how the data is loaded into PyTorch tensors. Observe how the DataLoader is defined. The DataLoader is used to load the data in batches during training.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the hyperparamaters\n",
    "batch_size = 16\n",
    "num_epochs = 15\n",
    "learning_rate = 0.001\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# Use PyTorch DataLoaders to load the data in batches\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           drop_last=True)\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\"> \n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <p style=\"text-align:center; margin:auto;\"><img src=\"images/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Create a multilayer perceptron by using the <code>Sequential</code> module with the following attributes: </p> <br>\n",
    "<p style=\" text-align: center; margin: auto;\">1. Use two hidden layers, both of size 64.</p>\n",
    "<p style=\" text-align: center; margin: auto;\">2. Attach a dropout layer to each hidden layer.</p>\n",
    "<p style=\" text-align: center; margin: auto;\">3. Use a ReLU activation for each hidden layer.</p>\n",
    "<p style=\" text-align: center; margin: auto;\">4. Create one output layer.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a multilayer perceptron by using the Sequential module. Add the following in sequence:\n",
    "# Two hidden layers of size 64\n",
    "# Dropout layers attached to the hidden layers\n",
    "# ReLU activation functions\n",
    "# One output layer\n",
    "\n",
    "############### CODE HERE ###############\n",
    "\n",
    "\n",
    "\n",
    "############## END OF CODE ##############\n",
    "\n",
    "\n",
    "def xavier_init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "\n",
    "net.apply(xavier_init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the loss function and the optimizer\n",
    "# Choose cross-entropy loss for this classification problem\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimize with stochastic gradient descent. You can experiment with other optimizers.\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">To train the network, run the following cell. Watch how the training and validation loss change for each epoch.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "######################\n",
    "# Network training and validation\n",
    "\n",
    "# Start the outer epoch loop (epoch = full pass through the dataset)\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    training_loss, validation_loss = 0.0, 0.0\n",
    "\n",
    "    # Training loop (with autograd and trainer steps)\n",
    "    # This loop trains the neural network\n",
    "    # Weights are updated here\n",
    "    net.train()  # Activate training mode (dropouts and so on)\n",
    "    for data, target in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        # Forward + backward + optimize\n",
    "        output = net(data)\n",
    "        L = loss(output, target)\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        # Add batch loss\n",
    "        training_loss += L.item()\n",
    "\n",
    "    net.eval()  # Activate eval mode (don't use dropouts and so on)\n",
    "    for data, target in val_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = net(data)\n",
    "        L = loss(output, target)\n",
    "        # Add batch loss\n",
    "        validation_loss += L.item()\n",
    "\n",
    "    # Take the average losses\n",
    "    training_loss = training_loss / len(train_loader)\n",
    "    val_loss = validation_loss / len(val_loader)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        \"Epoch %s. Train_loss %f Validation_loss %f Seconds %f\"\n",
    "        % (epoch, training_loss, val_loss, end - start)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Testing the neural network\n",
    "\n",
    "Now you can evaluate the performance of the trained network on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Activate eval mode (don't use dropouts and so on)\n",
    "net.eval()\n",
    "\n",
    "# Get test predictions\n",
    "predictions = net(X_test)\n",
    "\n",
    "# Print performance of the test data\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test.cpu().numpy(), predictions.argmax(axis=1).cpu().detach().numpy()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Improvement ideas\n",
    "\n",
    "You can improve this neural network by tuning network parameters such as the following:\n",
    "  - Architecture\n",
    "  - Number of layers\n",
    "  - Number of hidden neurons\n",
    "  - Choice of activation function\n",
    "  - Weight initialization\n",
    "  - Dropout\n",
    "  - Choice of optimizer function\n",
    "  - Learning rate\n",
    "  - Batch size\n",
    "  - Number of epochs\n",
    "  \n",
    "As you make changes, closely monitor the loss function and the accuracy on both training and validation to identify what changes improve your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, you built a basic neural network to process text data.\n",
    "\n",
    "--- \n",
    "## Next Lab: Introducing CNNs\n",
    "In the next lab in this module you will learn how to build a convolutional neural network to process hand written numbers."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

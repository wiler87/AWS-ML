{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/logo.png\" alt=\"AWS Logo\" width=\"400\" style=\"background-color:white; padding:1em;\" /></center> <br/>\n",
    "\n",
    "# Application of Deep Learning to Text and Image Data\n",
    "## Module 1, Lab 1, Notebook 2: Examining a Neural Network Architecture\n",
    "\n",
    "In this notebook, you will implement a minimum viable neural network to see the different architecture components.\n",
    "\n",
    "The simplest possible neural network architecture is logistic regression. This lab will cover data ingestion, how to define the model, loss function, and the optimization algorithm. Although modern deep learning frameworks can automate nearly all of this work, you will only using matrix multiplications and automatic differentiation to build, train, and test a logistic regression model.\n",
    "\n",
    "You will do the following:\n",
    "\n",
    "- Generate a simulated dataset\n",
    "- Use basic components of a neural network\n",
    "- Implement a neural network by using PyTorch\n",
    "- Train a neural network\n",
    "\n",
    "---\n",
    "\n",
    "You will be presented with activities throughout the notebook: <br/>\n",
    "\n",
    "|<img style=\"float: center;\" src=\"images/activity.png\" alt=\"Activity\" width=\"125\"/>| \n",
    "| --- | \n",
    "|<p style=\"text-align:center;\"> No coding is needed for an activity. You try to understand a concept, <br/>answer questions, or run a code cell.</p>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Index\n",
    "\n",
    "* [Simulated dataset](#Simulated-dataset)\n",
    "* [Neural network basics](#Neural-network-basics)\n",
    "* [Implementing a neural network with PyTorch](#Implementing-a-neural-network-with-PyTorch)\n",
    "* [Training of the neural network](#Training-of-the-neural-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Simulated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, you will train a neural network on a dataset that is randomly generated. The dataset will have two classes, and you will train the neural network to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install -U -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the sample data\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Specify settings, including how many examples to extract\n",
    "X, y = make_circles(\n",
    "    n_samples=750, shuffle=True, random_state=42, noise=0.05, factor=0.3\n",
    ")\n",
    "# ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_dataset(X, y, title):\n",
    "\n",
    "    # Activate the Seaborn visualization\n",
    "    sns.set()\n",
    "\n",
    "    # Plot both classes: Class 1 is blue, Class 2 is red\n",
    "    plt.scatter(X[y == 1, 0], X[y == 1, 1], c=\"blue\", label=\"Class 1\")\n",
    "    plt.scatter(X[y == 0, 0], X[y == 0, 1], c=\"red\", label=\"Class 2\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_dataset(X, y, title=\"Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to build a neural network that can differentiate between the two classes in the dataset. The simplest neural network that can tackle this problem is a single-layer neural network, basically a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Use a GPU resource if available; otherwise, use CPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# To work with PyTorch, you need to convert the dataset to tensors first\n",
    "X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape((-1, 1)).to(device)\n",
    "\n",
    "# why \"y\" uses reshape((-1, 1))???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Neural network basics\n",
    "\n",
    "The fundamental building blocks of neural networks are *neurons*, which are functions that are followed by an activation function. It's common for the initial function to be a linear regression and the activation function to be a sigmoid.\n",
    "\n",
    "<img style=\"float: center;\" src=\"images/single_layer.png\" alt=\"Neuron with activation function\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first goal is to implement a simple neural network with one neuron that uses the sigmoid function as the activation function to predict class 1 or class 2 from the sample dataset.\n",
    "\n",
    "As an equation, this would look like the following: For some input $\\mathbf{X}$ and output $\\mathbf{y}$, the logistic regression model is defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = \\sigma(\\mathbf{X}\\mathbf{w} + \\mathbf{b}),\n",
    "$$\n",
    "\n",
    "with some initial choices for the parameters, $\\mathbf{w}$ weights matrix and bias $\\mathbf{b}$.\n",
    "\n",
    "You don't know what the best values for $\\mathbf{w}$ and $\\mathbf{b}$ are, so initialize the weights matrix $\\mathbf{w}$ at random with zero mean and standard deviation 1, and let the bias take the initial value 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the logistic regression\n",
    "def log_reg(X, w, b):\n",
    "    return torch.sigmoid(torch.matmul(X, w) + b)\n",
    "\n",
    "\n",
    "# Define the basic neural network\n",
    "net = log_reg\n",
    "\n",
    "# Initialize for w and b\n",
    "w = torch.normal(0, 1, size=(2, 1), requires_grad=True).to(device)\n",
    "b = torch.zeros(1, requires_grad=True).to(device)\n",
    "\n",
    "# To test the neural net, pass in an example data point\n",
    "print(\n",
    "    f\"For datapoint 0, the probability of being class 1 is {float(net(X[0], w, b).item()):.2f}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a basic single-layer neural network that hasn't been trained yet. Ultimately, the goal is to find the best possible values for $\\mathbf{w}$ and $\\mathbf{b}$. You need to choose a loss function that systematically evaluates how good the predictions are and an optimization method that updates the weight and bias values. You can do this manually, but PyTorch has built-in features to do this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import system library and append path\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"..\")\n",
    "\n",
    "# Import utility functions that provide answers to challenges\n",
    "from MLUDTI_EN_M1_Lab1_quiz_questions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h3><i>Try it yourself!</i></h3>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"images/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">It's time to check your knowledge. To load the question, run the following cell.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to display the question and check your answer\n",
    "question_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implementing a neural network with PyTorch\n",
    "\n",
    "Now you need to build, train, and validate a neural network in PyTorch. With PyTorch, you can list the different layers and activation functions that you want to use, in the sequence that you want them to run. For more information, see the [PyTorch documentation](https://pytorch.org/docs/stable/index.html).\n",
    "\n",
    "You can use the `Sequential()` function to define the functions in the order that you want them to run. The first row refers to the first function to be used, followed by an activation function. You can chain together as many functions as you want to create your final model output. For more information, see [torch.nn Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) in the PyTorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Create a sequential container that chains a linear regression function with a sigmoid activation function\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 1),  # Linear layer-1 with 1 out_features and input size 2\n",
    "    nn.Sigmoid(),  # Sigmoid activation function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Before you continue, you need to initialize PyTorch values. This is important for the same reason that you initialized values for $\\mathbf{w}$ and $\\mathbf{b}$.\n",
    "\n",
    "Picking the starting point is critical. Researchers have developed several initialization strategies that you can use. The _Xavier initialization_ is commonly used because it can keep the scale of gradients roughly the same in all the layers, which helps to keep the gradient from vanishing or exploding.\n",
    "\n",
    "For a list of initializers, see the [PyTorch documentation](https://pytorch.org/docs/stable/nn.init.html). For information about Xavier initialization, see [Xavier Initialization](https://d2l.ai/chapter_multilayer-perceptrons/numerical-stability-and-init.html#xavier-initialization) on the Dive into Deep Learning site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the weights with an Xavier initializer\n",
    "def xavier_init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "# Apply the initialization to the sequential network that you created earlier\n",
    "net.apply(xavier_init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loss function\n",
    "\n",
    "Now that you have set up a neural network, you need to select a loss function to quantify how good a given selection of parameters are. Many loss functions exist, and you need to select one that suits the prediction problem, which is classificiation in this case. For information about loss functions, see the [Dive into Deep Learning site](https://d2l.ai/chapter_linear-regression/linear-regression.html#loss-function).\n",
    "\n",
    "Binary cross-entropy loss (log loss) is a loss function that is commonly used for binary classification:\n",
    "\n",
    "```python\n",
    "loss = nn.BCELoss()\n",
    "```\n",
    "\n",
    "During the training of the neural network, the initial model parameters will be updated until model predictions fit the data sufficiently well. One way to control for this is by setting the number of epochs that the model trains for. If large changes occur in the output quality, you can increase the number of epochs and train the model for longer.\n",
    "\n",
    "For a full list of supported loss functions, see [torch.nn Loss Functions](https://pytorch.org/docs/stable/nn.html#loss-functions) in the PyTorch documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization method\n",
    "Each update requires taking the gradient of the loss function with respect to the parameters. Automatic differentiation is used to compute the gradient, and given this gradient, each parameter is updated in the direction that might reduce the loss.\n",
    "\n",
    "The `torch.optim` module provides necessary optimization algorithms for neural networks. You can use an optimizer to train a network by using the stochastic gradient descent (SGD) method and setting the learning rate at 0.001. For more information, see [Stochastic Gradient Descent](https://d2l.ai/chapter_optimization/sgd.html) on the Dive into Deep Learning site.\n",
    "\n",
    "```python\n",
    "from torch import optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "Three lines of code are required to perform a gradient descent update:\n",
    "\n",
    "```\n",
    "loss.backward() # Compute updates for each parameter\n",
    "optimizer.step() # Make the updates for each parameter\n",
    "optimizer.zero_grad() # Clean-up step for PyTorch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training of the neural network\n",
    "\n",
    "Now that you have all the components, you can create a loop that takes a given set of parameter values, creates outputs, evaluates the performance, and updates the parameters accordingly until it has completed a set number of iterations (epochs). Repeating the training for more epochs should further improve the model. However, some additional considerations impact how much the model will improve. One example is the number of data points that you want to evaluate per iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Start by looking at the basic network again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print network\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, specify the loss function, optimization method, and how many epochs (loops) are run to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs = 50  # Total number of epochs (loops)\n",
    "\n",
    "# Define the loss. Because you used sigmoid in the last layer, use nn.BCELoss.\n",
    "# Otherwise, you could have used nn.BCEWithLogitsLoss. This loss combines a sigmoid layer and the BCELoss in a single class.\n",
    "loss = nn.BCELoss(reduction=\"none\")\n",
    "\n",
    "# Define the optimizer, SGD with learning rate\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it is time for training! \n",
    "\n",
    "Training will run through the dataset 50 times, and print training and validation losses after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    training_loss = 0\n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    output = net(X)\n",
    "    L = loss(output, y).sum()\n",
    "    training_loss += L.item()\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "    training_loss = training_loss / len(y)\n",
    "    train_losses.append(training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training has completed, you can plot the training and validation loss plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.title(\"Loss vs. Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the loss (errors) decreases as the training process continues, as expected.\n",
    "\n",
    "One final step is to compare this plot to the validation loss to see if it is overfitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, you learned about training a neural network. You should now understand the basic steps of building a neural network and how to evaluate its performance.\n",
    "\n",
    "--- \n",
    "## Next lab\n",
    "In the next lab, you will learn about the multilayer perceptron, which is the simplest feed-forward neural network architecture, and how to use dropout layers to prevent overfitting."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
